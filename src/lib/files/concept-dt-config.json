{
	"phases": [
		{
			"phase": "Preparation",
			"description": "You cannot start blind. Preparation and boundary conditions",
			"blocks": [
				{
					"title": "Requirements Analysis",
					"number" : "01",
					"content": "The concept of a digital twin is very broad. Therefore, when developing new digital twin applications, it is always important to pay attention to relevant user stories. This primarily revolves around the question of which processes the digital twin should support. Through requirements analysis of the various use cases, it is determined what data and information are required and what functionalities and tools may need to be developed.",
					"persons": ["Consultant", "User Experience Designer", "Stakeholder"],
					"image": "https://storage.googleapis.com/ahp-research/projects/sogelink/hackathon/images/requirementanalyse.jpg"
				},
				{
					"title": "Data Collection",
					"number" : "02",
					"content": "",
					"components": [
						{
							"subtitle": "Data collection",
							"text": "In a geoinformation project, the data collection phase is a critical component that involves gathering, acquiring, and recording spatial and attribute data related to the geographical area of interest. This phase is essential for creating accurate and up-to-date geospatial databases. This can involve using GPS devices, survey instruments, remote sensing equipment, or other specialized tools. Field surveys may be necessary to collect attribute data, such as land use information, infrastructure details, or environmental characteristics.",
							"icon": ""
						},
						{
							"subtitle": "Open data",
							"text": "Besides gathering new data, gathering open source datasets plays an important role. Using open geodata, also known as open geospatial data, involves accessing and leveraging geographic information that is freely available to the public without restrictions or cost. These datasets are typically made available by governments, research institutions, and organizations for various purposes, such as research, analysis, application development, and community engagement",				
							"icon": ""
						}
					],
					"persons": ["Data analist", "Data manager", "Researcher"],
					"image": "https://storage.googleapis.com/ahp-research/projects/sogelink/hackathon/images/datacollectieLekdijk_geotop_boringen_druk-1.png"
				},
				{
					"title": "Technical Platform",
					"number" : "03",
					"content": "The technical platform is the core for enabling the creation, operation, and effectiveness of the digital twin. All the data and visualization components of the digital twin run on the technical platform",
					"components": [
						{
							"subtitle": "Storage",
							"text": "The technical platform in a digital twin of the physical environment plays a crucial role in enabling the creation, operation, and effectiveness of the digital twin. The technical platform supports the data and visualization components of the digital twin. It should enable the creation of 2D and 3D visual representations of the physical environment, providing an intuitive way for users to interact with and understand the twin.",
							"icon": "https://miro.medium.com/v2/resize:fit:256/1*CHzvR53_W9FR2s1BQW9Bqg.png"
						},
						{
							"subtitle": "Deployment",
							"text": "Deployment of applications and services in a digital twin environment refers to the process of making them available for use. This involves installing, configuring, and activating the software components of the digital twin. The deployment process may vary depending on the specific application or service. The platform provides the necessary tools and configuration options to facilitate the deployment of the digital twin components. TeamCity is used for automating the deployment of applications and services.",
							"icon": "https://cdn.icon-icons.com/icons2/1381/PNG/512/teamcity_94405.png"
						},
						{
							"subtitle": "Hosting",
							"text": "Hosting refers to the process of storing and serving digital twin applications and services on a server or cloud platform. The hosting environment should provide the necessary resources and infrastructure to support the digital twin's data and visualization components. It should also ensure that the digital twin is accessible to users and can be used effectively. Hosting is organized via the Geodan AHP Cloud Platform that runs on a Kubernetes and Rancher.",
							"icon": "https://kubernetes.io/images/favicon.png"
						},
						{
							"subtitle": "Security",
							"text": "Protecting the data and ensuring the security and privacy of sensitive information are paramount. The platform must include robust security features to safeguard the digital twin against cyber threats and unauthorized access.",
							"icon": "https://services.geodan.nl/public/document/_/api/data/GEOD1593WEBS/logo/default"
						}
					],	
					"persons": ["Platform admin", "Architect", "Back-end eveloper"],
					"image": "https://d3caycb064h6u1.cloudfront.net/wp-content/uploads/2022/10/dataprocessing-scaled.jpg"
				}
			]
		},
		{
			"phase": "Data",
			"description": "Data is the fundamental building block",
			"blocks": [	
			{
				"title": "3D Data Formats",
				"number" : "04",
				"content": "3D data processing refers to the techniques and methodologies used to process data so that it can be visualized in 3D. The data should be position in the physical world with height, width, and depth. 3D data can be stored in various formats, such as point clouds, mesh models, voxel grids, or volumetric representations.",
				"components": [
					{
						"subtitle": "3D tiles",
						"text": "3D Tiles is an open standard for streaming massive 3D geospatial datasets, primarily used for visualizing and interacting with 3D content in web and mobile applications. It is an initiative developed by the Cesium team and supported by the OGC (Open Geospatial Consortium). 3D Tiles is designed to efficiently organize and transmit large 3D datasets, such as 3D models of cities, terrain data, or any complex 3D content. Geodan Research has developed custom tooling to process data into 3D Tiles, including <a href='https://github.com/Geodan/pg2b3dm', target='_blank'>pg2b3dm</a> and <a href='https://github.com/Geodan/i3dm.export', target='_blank'>i3dm.export</a>.",
						"icon": ""
					},
					{
						"subtitle": "BIM to GIS",
						"text": "Building Information Modeling (BIM) and Geographic Information Systems (GIS) are two distinct technologies used in the architecture, engineering, and construction (AEC) industry, as well as in urban planning and facility management. They serve different purposes but are often integrated to combine the detailed building information of BIM with the geospatial context of GIS. ",
						"icon": ""
					},
					{
						"subtitle": "GLB/glTF",
						"text": "GLB (GL Binary) and glTF (GL Transmission Format) are open standard file formats designed for 3D models and scenes. They are often used in computer graphics, 3D modeling, virtual reality, augmented reality, and web-based applications. Both formats are developed and maintained by the Khronos Group, a consortium of companies that create open standards for 3D graphics and media.",
						"icon": ""
					},
					{
						"subtitle": "OGC Web Map Services",
						"text": "OGC Web Map Services (WMS) is a standard protocol developed by the Open Geospatial Consortium (OGC) for sharing geospatial map data and images over the internet. WMS is widely used in the field of Geographic Information Systems (GIS) and web mapping to provide a standardized way for clients to request and receive map images from remote servers. ",
						"icon": ""
					}
				],
				"persons": ["Data analist", "Data manager", "ETL expert"],
				"image": "https://storage.googleapis.com/ahp-research/projects/sogelink/hackathon/images/data_auto-s_shutterstock_1343869916.jpg"
			},
			{
				"title": "3D Terrain",
				"number" : "05",
				"content": "<div>3D Terrain is essential in a Digital Twin. Digital Elevation Model (DEM) is one of the most common formats for elevation data and are usually provided in TIFF or ASC files. The data typically requires pre-processing to account for gaps (e.g. on water surfaces) or irregularities. The resulting data can then be processed into a 3D Terrain, or in more technical terms, a triangulated 3D mesh. A quantized 3D mesh is a special format that is suited for streaming massive terrain datasets for 3D visualization. <a href='https://github.com/mfbonfigli/gocesiumtiler' target='_blank'>GoCesiumTiler</a> is a useful tool to make such quantized terrain meshes.</div><img src='https://images.prismic.io/cesium/2015-12-18-terrain-obb-wireframe.png?auto=compress,format' />",
				"persons": ["Developer"],
				"image": "https://user-images.githubusercontent.com/538812/208296434-39bb50ec-7acf-4969-9f0a-546ee08138d9.png"
			},
			{
				"title": "Data Fusion",
				"number" : "06",
				"content": "Data fusion is the process of enhancing existing data by adding valuable, relevant, and supplementary information to it. This additional data can provide greater context, depth, and insights into the existing dataset, making it more valuable for analysis, decision-making, and other purposes. Data fusion can be applied to various types of data, including appending demographic information (e.g., age, gender, education) or geospatial data (e.g.,geographic boundaries).",
				"components": [
					{
						"subtitle": "Modelling",
						"text": "Data curation is the process of collecting, organizing, and managing data to ensure its quality, reliability, and usability over time. It involves various activities aimed at maintaining and enhancing the value of data assets, making them accessible, and preserving them for current and future use. Data curation often involves standardizing data formats, units, and terminology. This ensures that data can be easily understood and used by different people and systems. Ensuring data quality is a fundamental aspect of data curation. This involves checking data for accuracy, completeness, consistency, and reliability. Data quality issues may be identified and corrected during this stage.",
						"icon": ""
					},
					{
						"subtitle": "Machine learning",
						"text": "Machine learning plays a significant role in data curation by automating and improving the process of collecting, cleaning, organizing, and maintaining data. Data curation is essential for ensuring that data is reliable, accurate, and ready for analysis and decision-making. Machine learning techniques are applied to various aspects of data curation to enhance efficiency and data quality. Machine learning algorithms can analyze the statistical properties of data, helping to identify anomalies, missing values, and inconsistencies. For instance, machine learning models can detect outliers or data that deviates from expected patterns. Natural language processing (NLP) techniques and machine learning can standardize and transform text data, ensuring consistency in format and terminology.",
						"icon": ""
					},
					{
						"subtitle": "Data fusion",
						"text": "Data fusion involves bringing together data from different sources, which may include databases, sensors, external APIs, spreadsheets, and various file formats. This can include data in different structures, formats, and levels of granularity. Data from multiple sources may require transformation to ensure that it can be merged effectively. For example, text data may need to be tokenized, and numeric data might require scaling or normalization. In GIS and geospatial data curation, spatial alignment is crucial. This involves aligning data points on maps, reconciling coordinate systems, and handling overlaps or gaps in geographic data. Machine learning algorithms, such as ensemble methods, can be used to learn how to optimally combine data from multiple sources. These models can capture patterns and relationships in the data to make fusion decisions.",
						"icon": ""
					}
				],
				"persons": ["Modeller", "AI/ML expert", "ETL-expert"],
				"image": "https://storage.googleapis.com/ahp-research/projects/sogelink/hackathon/images/google3dtiles.PNG"
			},
			{
				"title": "Extract, Transform, Load",
				"number" : "07",
				"content": "",
				"components": [
					{
						"subtitle": "ETL pipelines",
						"text": "The data processing usually consists of multiple steps to get to the result. These steps are often automated in an ETL pipeline. ETL stands for Extract, Transform, and Load. The pipeline extracts data from a source, transforms it into a format that is suitable for the target system, and loads it into the target system. The ETL pipeline can be used to automate the data processing steps, ensuring that the data is always up-to-date and ready for use. <a href='https://www.mage.ai/' target='_blank'>Mage</a> is a tool that can be used to automate ETL processes.",
						"icon": "https://avatars.githubusercontent.com/u/69371472?s=280&v=4"
					}
				],
				"persons": ["Developer"],
				"image": "https://user-images.githubusercontent.com/538812/208296434-39bb50ec-7acf-4969-9f0a-546ee08138d9.png"
			},
			{
				"title": "Data Services and APIs",
				"number" : "08",
				"content": "Custom APIs, or Custom Application Programming Interfaces, are software interfaces developed for specific applications, systems, or services to enable communication, data exchange, and functionality between different components of a software ecosystem. These APIs are tailored to the unique requirements of a particular application or organization. Data exposure in the GIS (Geographic Information Systems) field refers to making geospatial data accessible, often to a wide audience or specific users, for various purposes. Data exposure involves sharing, publishing, and providing access to geographic information in a manner that enables data consumers to view, analyze, and utilize the data. GIS professionals and organizations publish geospatial data to make it available to others. This data can include maps, aerial imagery, satellite data, geospatial layers (e.g., land use, transportation networks, environmental data), and more. Data publishing involves making data accessible through online platforms, databases, web services, or public repositories.</div><div>Data exposure often comes with licensing terms and conditions. Organizations may specify how the data can be used, whether it can be redistributed, and any attribution requirements. Clear licensing and usage terms promote responsible data consumption.",
				"components": [
					{
						"subtitle": "pg_featureserv",
						"text": "<a href='https://github.com/CrunchyData/pg_featureserv' target='_blank'>pg_featureserv</a> is an API to communicate with PostgreSQL database. Execute SQL queries and return results in JSON format.",
						"icon": ""
					},
					{
						"subtitle": "GoParquet",
						"text": "A custom Geodan Research tool to query Parquet files. Parquet is a columnar storage format that is optimized for large datasets. It is especially useful to query large real-time datasets.",
						"icon": "https://miro.medium.com/v2/resize:fit:600/0*mB3erjnn6CpvHHHZ.png"
					},
					{
						"subtitle": "PostGraphile",
						"text": "PostGraphile offers a GraphQL API for PostgreSQL databases. GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools. PostGraphile is a tool that can be used to automatically generate a GraphQL API for a PostgreSQL database. It provides a powerful and flexible way to query and manipulate data in a PostgreSQL database. PostGraphile is used in the Geodan Research Data Exposure platform to provide a GraphQL API for the PostgreSQL database",
						"icon": "https://camo.githubusercontent.com/ee28dc15ebd25aad43a8e3566e208bcf6cdfb8d568ffb86c51a99ae2c1482ace/68747470733a2f2f63646e2e7261776769742e636f6d2f6772617068696c652f6772617068696c652e6769746875622e696f2f613632323566386333303532646635633237366563656632386165623063616465316165633136612f6c6f676f732f706f73746772617068696c652e6f7074696d697a65642e737667"
					}
				],
				"persons": ["Architect", "Developer", "User Experience Designer"],
				"image": "https://storage.googleapis.com/ahp-research/projects/sogelink/hackathon/images/dataexposureduisburgsmartcity.PNG"
			},
			{
				"title": "Data Curation",
				"number" : "09",
				"content": "Data curation is the process of collecting, organizing, and managing data to ensure its quality, reliability, and usability over time. It involves various activities aimed at maintaining and enhancing the value of data assets, making them accessible, and preserving them for current and future use. Data curation often involves standardizing data formats, units, and terminology. This ensures that data can be easily understood and used by different people and systems. Ensuring data quality is a fundamental aspect of data curation. This involves checking data for accuracy, completeness, consistency, and reliability. Data quality issues may be identified and corrected during this stage.",
				"components": [
					{
						"subtitle": "Data Management",
						"text": "Many different data management systems exist. From Geodan, we have experience with CKAN, which is an open-source data management system that provides a flexible way to manage and publish data along with its relevant metadata. It is used by governments, research institutions, and organizations worldwide to share and manage data.",
						"icon": "https://upload.wikimedia.org/wikipedia/en/e/e2/CKAN_Logo_full_color.png"
					},
					{
						"subtitle": "Real-time data",
						"text": "Real-time data refers to data that is collected and processed in real-time, as it is generated. This data is typically used for monitoring and analysis of real-world systems, such as traffic, weather, or environmental conditions. Real-time data is often used in digital twins to provide up-to-date information about the physical environment. The quantities of data are huge and proper data management is crucial. Currently, we are making use of DuckDB to manage real-time data.",
						"icon": "https://duckdb.org/images/logo-dl/DuckDB_Logo.png"
					}
				],	
				"persons": ["Modelleur", "AI/ML expert"],
				"image": "https://storage.googleapis.com/ahp-research/projects/sogelink/hackathon/images/data-processing-geoinformatie.jpg"
			}
		]
	},
	{
		"phase": "Digital Twin",
		"description": "Integrate everything in a real-world environment",
		"blocks": [
			{
				"title": "3D Visualization",
				"number" : "10",
				"content": [
					{
						"subtitle": "Web viewer (CesiumJS/MapLibre)",
						"text": "Both CesiumJS and MapLibre are valuable tools for creating web viewers that present digital twin visualizations, geospatial data, and 3D models in an interactive and user-friendly manner. They provide an accessible foundation for a wide range of applications, from urban planning and architecture to virtual tourism and environmental monitoring. The choice between the two often depends on specific project requirements and development preferences.",
						"icon": "plaatje"
					},
					{
						"subtitle": "Cesium as technical platform",
						"text": "The technical plaform of this digital twin is Cesium. Cesium is an open-source geospatial platform and a 3D visualization engine that enables the creation of interactive, high-performance 3D maps and geospatial applications. It is primarily known for its capabilities in rendering geospatial data in 3D, making it a powerful tool for creating digital twins, virtual globes, and other geospatial applications.",
						"icon": "plaatje"
					},
					{
						"subtitle": "Photoreal viewer (Unity/Unreal/NVIDIA)",
						"text": "Photoreal viewers like Unity, Unreal Engine, and NVIDIA Omniverse are instrumental in creating visually stunning and interactive digital twin experiences. They are used in various industries, including architecture, engineering, gaming, film production, and virtual training, to provide immersive and realistic simulations of complex 3D environments and models. The choice of platform often depends on the specific requirements of a project, such as the level of graphical fidelity, interactivity, and the target platform (e.g., desktop, VR, AR).",
						"icon": "plaatje"
					},
					{
						"subtitle": "Analyis dashboards",
						"text": "An analysis dashboard in the context of digital twin visualization is a user interface that provides tools and features for monitoring, analyzing, and making data-driven decisions based on the data and simulations of a digital twin. These dashboards are designed to facilitate real-time insights, visualization of complex data, and interactive analysis of a physical or digital system's behavior. ",
						"icon": "plaatje"
					},
					{
						"subtitle": "Real-time (IoT) data visualization",
						"text": "Real-time IoT data visualization in a digital twin is the process of presenting and analyzing data from Internet of Things (IoT) devices, sensors, and other sources within a digital twin environment. It enables users to monitor and understand the current state of a physical system, asset, or environment in real-time. This capability is crucial for gaining insights, making informed decisions, and responding to changes as they happen. ",
						"icon": "plaatje"
					},
					{
						"subtitle": "Custom tools and interactive modules",
						"text": "Custom tools and interactive modules in the context of digital twins are software components or features specifically designed to enhance the functionality and user experience within a digital twin environment. These tools and modules are tailored to the unique needs and requirements of a particular digital twin application. They often enable users to interact with and manipulate the digital twin's data, simulations, and models in a more intuitive and efficient manner.",
						"icon": "plaatje"
					}
				],
				"persons": ["Architect", "Front-end Developer"],
				"image": "https://storage.googleapis.com/ahp-research/projects/sogelink/hackathon/images/banner2.jpg"
			},
			{
				"title": "User interfaces and tools",
				"number" : "11",
				"content": "User interfaces enable users to interact with and manipulate the digital twin's data, simulations, and models in a more intuitive and efficient manner. User interfaces are designed to facilitate specific tasks or workflows. They can be tailored to the unique needs and requirements of a particular digital twin application. User interfaces can be designed for various purposes, such as data visualization, analysis, and simulation. They can also be used to provide tools and features for monitoring, controlling, and managing a physical system or environment. User interfaces are often designed to facilitate specific tasks or workflows. They can be tailored to the unique needs and requirements of a particular digital twin application. User interfaces can be designed for various purposes, such as data visualization, analysis, and simulation. They can also be used to provide tools and features for monitoring, controlling, and managing a physical system or environment.",
				"components": [
					{
						"subtitle": "Layer library",
						"text": " ",
						"icon": ""
					},
					{
						"subtitle": "Bookmarks and Projects",
						"text": " ",
						"icon": ""
					},
					{
						"subtitle": "Measure tool",
						"text": " ",
						"icon": ""
					},
					{
						"subtitle": "Stories",
						"text": " ",
						"icon": ""
					}
				],
				"persons": ["Architect", "Developer"],
				"image": "https://storage.googleapis.com/ahp-research/projects/sogelink/hackathon/images/banner2.jpg"
			},
			{
				"title": "Real-time insight",
				"number" : "12",
				"content": "Examples: Traffic/parking places Nantes. Ship animation.",
				"persons": ["Architect", "Developer"],
				"image": "https://storage.googleapis.com/ahp-research/projects/sogelink/hackathon/images/data_auto-s_shutterstock_1343869916.jpg"
			},
			{
				"title": "Data Analysis and Simulation",
				"number" : "13",
				"content": "TKI dikes, Moerdijkbrug",
				"persons": ["Data analyst", "Information analist"],
				"image": "https://d3caycb064h6u1.cloudfront.net/wp-content/uploads/2022/10/dataprocessing-scaled.jpg"
			},
			{
				"title": "Feedback and Use",
				"number" : "14",
				"content": "<p>Feedback and use are crucial aspects of their development and application. Digital twins are dynamic, data-driven models of real-world objects or environments, and their effectiveness relies on continuous feedback, usage, and improvement. Some examples of feedback:<br>1 Data Quality and Feedback:<br>Data Validation: Feedback is essential for ensuring the accuracy and quality of the data used in digital twins. Users can provide feedback on data discrepancies or inaccuracies, which can be used to refine the data sources and data processing pipelines.<br>Data Updates: Real-world data changes over time. Users can offer feedback on outdated information, prompting the update of data in the digital twin to reflect the most current state of the physical environment.<br> 2 Data Validation: User Engagement and Feedback:<br>User Experience: Feedback from users about the usability and user experience of digital twins is valuable. This can lead to user interface improvements, enhanced interactivity, and a more intuitive design.<br>Feature Requests: Users often have specific needs and requirements. Feedback can include requests for additional features, tools, or data layers, which can guide the development of the digital twin to meet these needs.<br>3 Accessibility and Inclusivity:<br>Inclusive Feedback: Ensuring that digital twins are accessible to individuals with disabilities is an important consideration. Feedback can help identify accessibility challenges and drive improvements.</p>",
				"persons": ["Consultant", "User Experience Designer"],
				"image": "https://storage.googleapis.com/ahp-research/projects/sogelink/hackathon/images/geoinfo-wur.jpg"
			}
		]
	}
]}